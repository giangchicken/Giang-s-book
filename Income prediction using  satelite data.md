# **Summary of the Paper: "Using Publicly Available Satellite Imagery and Deep Learning to Understand Economic Well-Being in Africa"**

---


## **Introduction**

The paper addresses the **need for accurate local-level measurements of economic well-being** to support effective policy-making and targeted programs in developing regions, particularly **in Africa**. Traditional survey methods for measuring asset wealth are often **infrequent and costly**, leading to **gaps in data availability**. The authors propose an approach using **publicly available multispectral satellite imagery** combined with **deep learning** to predict **asset wealth** across approximately **20,000 African villages**, offering a **timely and scalable method** for economic estimation. 

> Output of model is **wealth index**.
---

## **Results**

The study demonstrates that **deep learning models trained on satellite imagery** can **explain about 70% of the variation** in ground-measured village wealth, **outperforming previous benchmarks** that used high-resolution imagery. Furthermore, these models can explain **up to 50% of the variation in district-aggregated wealth changes** over time. The **errors in satellite-based estimates** are also **comparable to those in existing ground data**, indicating the **reliability** of satellite imagery as an economic measurement tool.

### **Key Findings to Note**
![Economic data from household surveys are infrequent in many African countries](./images/Screenshot%202024-10-26%20220411.png)

>*Figure Description:*  
**Figure 1**: illustrates the efficacy of satellite-based predictions in explaining **variations in survey-measured wealth estimates** across multiple African countries. The CNN model (MS+NL) predictions are validated against **ground-truth survey measures** (DHS and census data) using a 5-fold cross-validation approach. Key panels include:
>
>>- **(a)**:   Frequency of nationally representative household consumption expenditure or asset  wealth surveys across Africa, 2000–2016
>>- **(b)**:  Average household revisit rate for surveys and average location revisit rate for various resolutions of satellite imagery over time. Survey revisit rate, the average time elapsed between observations of a given household in nationally representative expenditure or wealth surveys, is calculated as number of total person-days (population×365) divided by the number of person-days observed in a given year. Satellite revisit rate estimates are calculated as the number of days in a year divided by the average number of images taken in a year across 500 randomly sampled DHS clusters in African countries, only counting images with <30% cloud cover.

---
![Satellite-based predictions explain the majority of variation in survey-based wealth estimates in all countries, and validate well against independent ground measures](./images/Screenshot%202024-10-26%20230143.png)

>*Figure Description:*  
**Figure 2: Satellite-based predictions explain the majority of variation in survey-based wealth estimates in all countries, and validate well against independent ground measures**. Key panels include:
>
>>- **(a)**: Shows predicted wealth index vs. survey-measured wealth index across all locations and survey years. Each point represents a survey enumeration area (roughly a village), and satellite predictions are generated by the CNN MS+NL model for each country using a model trained outside that country. The red *R²* values report pooled observation fit, while black *R²* values are averaged within country-years.
>>- **(b)**: Similar to (a) but aggregated to the **district level**.
>>- **(c)**: Displays average *R²* over survey years at the **village level**, by country.
>>- **(d)**: Displays average *R²* over survey years at the **district level**, by country.
>>- **(e)**: Compares DHS-based asset wealth to independent census-based measures at the **district level**, for available census measures taken within four years of the DHS survey.
>>- **(f)**: Compares CNN-predicted wealth indices (trained on held-out countries) with independent **census-based asset wealth** measures at the district level.

>>*Note:* In panels (b), (e), and (f), the reported *R²* values are weighted and unweighted by the **number of villages** contributing to each district-level average, with dot size representing the number of villages.

---
![Performance by model and across different samples.](./images/Screenshot%202024-10-26%20232436.png)

>*Figure Description:*  
**Figure 3: Performance by Model and Across Different Samples**. This figure showcases the predictive performance of satellite-based wealth estimates, evaluated across multiple machine learning models and different sample divisions. Key panels include:
>
>>**(a)**: Model Comparison
>>- Performance (R²) of satellite predictions is assessed using five different models:
>>>- **NL**: Nightlights imagery only.
>>>- **MS**: Landsat multispectral imagery only.
>>>- **Transfer**: Transfer learning on nightlights combined with RGB Landsat imagery.
>>- Each grey line represents performance on a held-out country-year, while:
>>>- **Black lines and text** denote the average performance across all country-years.
>>>- **Red lines and text** show the R² for the pooled sample.

>>**(b)**:  Similar to Panel a but evaluated on held-out villages within the same country, testing the model’s intra-country generalization ability.
>
>>**(c)**:  Evaluates model performance based on the amount of training data utilized, showing how different sample sizes impact the accuracy of predictions.
>
>>**(d)**: Urban vs. Rural
>>- Performance of the **CNN MS+NL model** across urban (blue) and rural (red) regions in held-out countries.
>>- The model is trained on the entire training dataset, then applied separately to urban and rural clusters, with:
>>>- Each dot representing a cluster, displaying **predicted** (x-axis) vs **ground-measured** (y-axis) wealth index.
>>>- Density plots depict the distribution of wealth index predictions in each region.
>
>>**(e)**: Wealth Distribution
>>- Performance analyzed across different wealth percentiles, with experiments run separately for increasing percentages of available clusters. 
>>- For example, an x-axis value of 4 indicates that clusters below the 40th wealth percentile were included in the test set, allowing examination of model accuracy across wealth distributions.
>
>>This figure comprehensively illustrates how different models and varying data samples affect the accuracy and reliability of satellite-based economic predictions, highlighting the robustness of CNN-based approaches across multiple conditions.

---

![Satellite predictions of ground-measured changes in wealth over time](./images/Screenshot%202024-10-26%20234704.png)

>*Figure Description:*  
**Figure 4: Satellite Predictions of Ground-Measured Changes in Wealth Over Time**. This figure presents the performance of satellite-based models in predicting changes in wealth over time, as measured at the village and district levels. Key panels include:
>
>>**(a)**: Village-Level Wealth Change Prediction
>>- The model predicts an **index of change in wealth** at the village level, based on household asset changes aggregated to form a village-wide wealth index.
>>- The plot shows the model's accuracy in predicting these changes over time, providing insights into temporal economic trends at the local level.
>
>>**(b)**: District-Level Wealth Change Prediction
>>- Similar to Panel a but with predictions aggregated to the district level.
>>- Dot sizes represent the number of village observations in each district.
>>- **R² is reported in two ways**:
    - **Weighted** by the number of villages per district (R² = 0.51).
    - **Unweighted**, treating each district equally regardless of the number of villages.
>
>>**(c)**: Model Comparison by Imagery Type
>>- Cross-validated R² scores are presented for models trained on:
>>>- **MS (Multispectral)** imagery (red).
>>>- **NL (Nightlights)** imagery (blue).
>>>- **Combined MSNL (Multispectral and Nightlights)** imagery (green).
>>- All R² values reported in Panels c and d are **unweighted**, allowing for straightforward comparison across imagery types without adjusting for sample sizes.
>
>>This figure emphasizes the efficacy of satellite-based models in capturing temporal wealth dynamics, underscoring the value of combined multispectral and nightlights data in improving predictive accuracy.

---

![Using Satellite-Based Wealth Predictions in Downstream Tasks](./images/Screenshot%202024-10-27%20000226.png)

>*Figure Description:*  
**Figure 5: Using Satellite-Based Wealth Predictions in Downstream Tasks**. This figure demonstrates the utility of satellite-based wealth predictions in addressing downstream tasks such as environmental analysis and program targeting. Key panels include:
>
>>**(a)**: Temperature-Wealth Relationship
>>- Displays the **cross-sectional relationship** between average maximum temperature and wealth across survey locations.
>>- Comparisons are made using:
>>>- **Survey-based wealth data** (black line).
>>>- **Three satellite-based models** (colored lines), each representing different prediction approaches.
>>- Each line results from **100 bootstraps** of cross-sectional regressions, resampling villages with replacement.
>>- **Key Findings**:
>>>- The best-performing CNN-based models closely approximate the temperature-wealth relationships observed in survey data.
>>>- CNN-based models significantly outperform scalar nightlights models in replicating the observed temperature-wealth correlation.
>
>>**(b)**: Targeting Program Accuracy
>>- Assesses a **hypothetical targeting program** that provides assistance (e.g., cash transfers) to villages below a specified wealth threshold while excluding villages above it.
>>- **Targeting Accuracy**: Defined as the percentage of villages correctly included or excluded from the program, assuming survey-based ground data provide the "true" asset distribution.
>>- **Comparison of Model Accuracy**:
>>>- **MS+NL estimates**: Achieve a targeting accuracy of 81% when targeting households below the median wealth threshold.
>>>- **CNN Transfer model**: Achieves a 75% targeting accuracy.
>>>- **Scalar NL models**: Result in 62% targeting accuracy.
>
>>- **Note**: These estimates likely **understate true targeting accuracy**, as ground-measured data contain inherent noise.

---

![Spatial Extent of Imagery Enables Large-Scale Wealth Predictions](./images/Screenshot%202024-10-27%20001829.png)

>*Figure Description:*  
**Figure 6: Spatial Extent of Imagery Enables Large-Scale Wealth Predictions**. This figure illustrates the capacity of satellite imagery to generate wealth predictions across large geographic areas, with Nigeria as the focus. Key panels include:
>
>>**(a)**: Wealth Estimates Across Nigeria
>>- Shows **satellite-based wealth estimates** at the pixel level across Nigeria.
>>- This spatially detailed view enables insights into wealth distribution patterns on a national scale.
>
>>**(b), (d), (f)**:  Zoomed-in imagery over a specific region in **Southern Nigeria** (highlighted in **(a)**).
>>**Imagery Inputs**:
>>>- **(b)**: Nightlights (NL) data, capturing urban brightness and other light sources.
>>>- **(d)**: Multispectral (MS) imagery, offering detailed environmental and land use data.
>>>- **(f)**: Ground-truth socioeconomic data, used as inputs for model training.
>
>>**(c), (e), (g)**:  Model Predictions Using Different Inputs.
>>>- **(c)**: Predictions using only **nightlights (NL)** data.
>>>- **(e)**: Predictions using only **multispectral (MS)** imagery.
>>>- **(g)**: Predictions combining **both NL and MS** features.
>>- **Findings**:
>>>- In this Southern Nigeria region, the model shows a stronger reliance on **MS imagery** over NL data.
>>>- The model disregards light blooms caused by **gas flares** visible in the nightlights data **(b)**, which can skew predictions based solely on NL.
>
>>**(h)**: Population-Weighted Wealth Index Across Nigeria
>>**Deciles of the satellite-based wealth index** are displayed across Nigeria.
>>- The index is **population-weighted** using the Global Human Settlement Layer and aggregated at the **Local Government Area level**.
>>- This provides a refined view of wealth distribution adjusted for population density, offering insights into regional inequalities and resource needs.
>
>>This visualization highlights the effectiveness of combining NL and MS imagery for wealth predictions, particularly in regions where environmental factors may influence NL data quality, such as gas flaring.


---

## **Modeling Approach**

The authors utilize a **convolutional neural network (CNN)** architecture to predict **village-specific wealth measures** from satellite imagery. Key aspects of the model include:

- **Input Data**: The model uses **multispectral daytime imagery** from **Landsat (30m/pixel)** and **nighttime lights imagery** as inputs, both **temporally and spatially matched**.
- **Architecture**: The CNN is designed to **learn features from both daytime and nighttime imagery** in an **end-to-end training process**. Separate models are trained for each imagery type and are **combined in a final fully connected layer**.
- **Training Process**: Models are trained using a **mean squared error loss function**, optimized with the **Adam optimizer**, and incorporate **data augmentation** to prevent overfitting.

---

## **Data and Quality**

The study leverages both **satellite imagery** and **ground truth data**:

- **Satellite Imagery**: Publicly available **multispectral images** from **Landsat** and **nighttime lights data**. These are processed into **3-year median composites** to reduce cloud cover effects.
- **Ground Truth Data**: Asset wealth data sourced from **Demographic and Health Surveys (DHS)** (2009-2016) covering **500,000+ households** across **19,669 villages** in **23 African countries**. An **asset wealth index** is created using **principal component analysis (PCA)** based on household asset data.
- **Data Collection Challenges**: The study notes challenges from **random GPS displacement in survey data**, which introduces **noise** and affects **spatial accuracy** when matching with satellite imagery.

This comprehensive approach illustrates the **feasibility of using satellite imagery and deep learning** to generate **reliable economic estimates**, providing a valuable resource for **researchers and policymakers** in regions where data is scarce.

> Training data taken by Landsat 6/7/8

>[Read all paper](https://www.semanticscholar.org/paper/Using-publicly-available-satellite-imagery-and-deep-Yeh-Perez/83bd44a487ea02e19a27e9d77cd736dd4f5bcc00)
>
>[Code](https://github.com/chrisyeh96/africa_poverty_clean/tree/main)
---

## **Scraping Data with Google Earth Engine**
Google Earth Engine (GEE) is a powerful cloud-based platform for planetary-scale geospatial analysis. It provides extensive satellite data archives and a suite of tools for analyzing this data, making it an essential resource for environmental monitoring, agricultural assessments, urban planning, and more. Leveraging GEE for data scraping allows users to access and process high-resolution images efficiently without the need for massive local storage or computational power.

In this guide, we’ll explore how to use GEE to access and visualize satellite images, focusing specifically on data from the **Landsat** program. Landsat provides continuous data since 1972, offering multispectral images with spatial resolutions that are ideal for detecting changes in land cover, monitoring vegetation, mapping urban areas, and analyzing surface water.

### **Landsat Data Overview**
Landsat satellites capture images of Earth's surface with multispectral sensors that can detect various surface characteristics. Key features of Landsat data include:
- **Temporal Coverage**: Long-term, continuous data availability since 1972.
- **Spatial Resolution**: 30 meters per pixel, with select bands at 15 meters.
- **Spectral Bands**: Includes visible (RGB), near-infrared, shortwave-infrared, and thermal bands, allowing diverse applications in environmental analysis.

>[For more infomations](https://developers.google.com/earth-engine/datasets/catalog/LANDSAT_LC08_C02_T2_L2#description)
>
>[GGE Tutorial](https://www.youtube.com/@giswqs)

Landsat data is freely available on GEE and can be accessed and filtered by date, location, cloud cover, and other parameters. Here’s an example of a Landsat image captured in Hà Nội - the capital of Việt Nam, showcasing the rich details of the terrain:

```
import ee
import folium

# Xác minh tài khoản GGE
ee.Authenticate()
# Khởi tạo Google Earth Engine
ee.Initialize(project="project-name") #Create project on GGE web


# Hàm áp dụng scale factor cho các băng Landsat
def apply_scale_factors(image):
    optical_bands = image.select('SR_B.').multiply(0.0000275).add(-0.2)
    thermal_bands = image.select('ST_B.*').multiply(0.00341802).add(149.0)
    return image.addBands(optical_bands, None, True).addBands(
        thermal_bands, None, True
    )

# Hàm lấy ảnh Landsat với các băng bổ sung như NIR và SWIR
def get_landsat_image(lat, lon, year, scale):
    # Lọc ảnh Landsat theo năm và vị trí
    landsat = ee.ImageCollection('LANDSAT/LC09/C02/T1_L2') \
        .filterBounds(ee.Geometry.Point([lon, lat])) \
        .filterDate(f'{year}-01-01', f'{year}-12-31') \
        .sort("CLOUD_COVER").map(apply_scale_factors) \
        .select(["SR_B4", "SR_B3", "SR_B2", "SR_B5", "SR_B6", "SR_B7"]) \
        .first()

    # Chọn các băng RGB, NIR, và SWIR
    image = landsat.select(['SR_B4', 'SR_B3', 'SR_B2', 'SR_B5', 'SR_B6', 'SR_B7'])

    return image

# Hàm để trực quan hóa ảnh với các băng bổ sung
def visualize_landsat(lat, lon, year, scale):
    # Lấy ảnh Landsat tại vị trí và năm mong muốn
    image = get_landsat_image(lat, lon, year, scale)

    # Tạo bản đồ Folium
    my_map = folium.Map(location=[lat, lon], zoom_start=15)

    # Thêm các băng RGB vào bản đồ
    folium.TileLayer(
        tiles=image.getMapId({'bands': ['SR_B4', 'SR_B3', 'SR_B2'], 'min': 0, 'max': 0.3})['tile_fetcher'].url_format,
        attr='Google Earth Engine - RGB',
        overlay=True,
        name='Landsat RGB'
    ).add_to(my_map)

    # Thêm các băng NIR (Near-Infrared) vào bản đồ
    folium.TileLayer(
        tiles=image.getMapId({'bands': ['SR_B5'], 'min': 0, 'max': 0.3})['tile_fetcher'].url_format,
        attr='Google Earth Engine - NIR',
        overlay=True,
        name='Landsat NIR'
    ).add_to(my_map)

    # Thêm các băng SWIR (Short-Wave Infrared) vào bản đồ
    folium.TileLayer(
        tiles=image.getMapId({'bands': ['SR_B7'], 'min': 0, 'max': 0.3})['tile_fetcher'].url_format,
        attr='Google Earth Engine - SWIR',
        overlay=True,
        name='Landsat SWIR'
    ).add_to(my_map)

    # Thêm một điểm đánh dấu vị trí
    folium.Marker([lat, lon], popup=f'Lat: {lat}, Lon: {lon}',
                  icon=folium.Icon(color='red', icon='info-sign')).add_to(my_map)

    # Tính toán vùng buffer theo scale
    point = ee.Geometry.Point([lon, lat])
    buffer = point.buffer(scale * 224).bounds()  # Buffer tính theo scale

    # Chuyển đổi vùng buffer thành tọa độ để vẽ hình chữ nhật trên bản đồ
    region = buffer.coordinates().get(0).getInfo()

    # Vẽ hình chữ nhật xung quanh vùng ảnh theo scale
    folium.Polygon(locations=[[coord[1], coord[0]] for coord in region],
                   color='blue', fill=True, fill_opacity=0.2).add_to(my_map)

    # Thêm control layer cho phép bật/tắt các lớp ảnh
    folium.LayerControl().add_to(my_map)

    # Hiển thị bản đồ
    return my_map

# Đầu vào từ người dùng
lat = 21.0285  # Ví dụ: Hà Nội
lon = 105.8542
year = 2024
scale = 30  # Đơn vị pixel

# Hiển thị ảnh Landsat và vùng sample trên bản đồ
visualize_landsat(lat, lon, year, scale)

```

Street map                 |  Landsat (RGB)
:-------------------------:|:-------------------------:
![Street map](./images/Screenshot%202024-10-27%20025057.png)   |  ![Landsat](./images/Screenshot%202024-10-27%20025440.png)

NIR map                    |  SWIR map
:-------------------------:|:-------------------------:
![Street map](./images/Screenshot%202024-10-27%20034652.png)   |  ![Landsat](./images/Screenshot%202024-10-27%20034723.png)

### **Scraping Data & Processing Data**

The code for scraping and data processing can be found hear, so this section will present notes and describe this code.
**Descripion**:
- Each pixel is a square with an edge length set SCALE meters (SCALE can be found in the code). Then, 127 pixels are taken on each opposite side to create an image with a shape of 224x224.
- The data is collected from a multi-satellite dataset (LANDSAT 6/7/8/9) to reduce noise, minimize missing values, and lessen the impact of clouds. Thus, one location may have several images. The article suggests that using the median of these images can effectively reduce the influence of clouds.

**Notes**:
- Prepare data to include coordinaté (latitude, longitude), year and country.
![Needs preparation](./images/Screenshot%202024-10-29%20143143.png)
---

## **Running Trained CNN models**
---
## **Experimenting with Data in Việt Nam**
---





